\chapter{Introduction}
This paper begins by exploring the research background for several key reasons:

Firstly, the focus of this study, Human-Computer Interaction (HCI), is an interdisciplinary domain\cite{paper1}\cite{paper2}\ that necessitates a certain degree of understanding in multiple fields from the reader. By presenting a comprehensive research background, aiming to enhance the reading experience and facilitate a more profound comprehension and reflection on the study's theme.

Secondly, the dissection and exploration of the research theme serve to illustrate the logical progression of this paper more effectively, thereby validating the posed research questions and underscoring the importance of this study.

Lastly, this paper is more than a mere report on the research topic. It encompasses a summary of the extensive efforts undertaken during author's master course. The journey to a clear and definitive research topic included numerous investigations and trials across various fields, an aspect I deem essential to acknowledge and explain within this paper.

\section{Structure}
Chapter 1: Introduction
This chapter primarily provides an introduction to the background knowledge related to the research topic of this study. It encompasses areas such as human-computer interaction, virtual reality and spatial computing, multisensory integration and haptic perception, as well as agency and artificial intelligence.

Chapter 2: Related Works
In this chapter, a comprehensive review of relevant literature on haptic feedback is conducted. The review summarizes previous studies and explores research projects that share conceptual and design similarities with the current research topic.

Chapter 3: Concept Design and Implementation
Chapter 3 presents multiple design iterations leading to the conceptualization and implementation of the MotionPerformer system. The feasibility and potential applications of the system are demonstrated through preliminary experiments.

Chapter 4: Proof of Concept
Two experiments are designed to validate the effectiveness of the MotionPerformer system in terms of force feedback performance and its impact on enhancing agency and user experience.

Chapter 5: Discussion
Chapter 5 analyzes and discusses the research objectives based on the results obtained from the experiments conducted in this study.

Chapter 6: Conclusion
Chapter 6 concludes the research paper and provides a summary of the findings and insights gained throughout the entire master's program.

\section{Humanâ€“computer Interaction and Virtual Reality}
\subsection{The Influence of VR Technology on HCI}
Since the concept of Human-Computer Interaction (HCI) was popularized in 1983, HCI researchers have been persistently investigating the proposition of how computers can interact better with humans. From the outset, this theory emphasized that the mode of interaction between computers and humans is different from other tools with limited purposes. Due to its extensive range of applications, the interaction between computers and humans has been analogized to the conversation process between humans.

HCI is defined within the computing community\cite{paper3} as "Human-computer interaction is a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them." Following this definition from the Association for Computing Machinery (ACM), it has been declared that this discipline is also associated with technological developments in computing, human factors disciplines, and even engineering and design methods.

Although the history of HCI's development can be divided in various ways\cite{paper4}, there are a few milestone nodes. In early computer systems, batch processing\cite{paper5e} and Command-Line Interface (CLI) development introduced the concept of early real-time interaction\cite{paper6e}. The advent of Graphical User Interfaces (GUIs), keyboards, mice, and robust underlying infrastructure marked the birth of HCI. The evolution of pervasive computing\cite{paper7} greatly propelled the development of both computers and the concept of HCI.

Nowadays, with the remarkable development of the internet, smartphones, and digital technology, HCI has expanded into areas such as web design and mobile application interaction design. Modern HCI doesn't merely seek usability, it also includes attributes such as user experience\cite{paper8e}, user engagement, and social computing. 

Many papers and articles\cite{paper9} have mentioned that mixed reality technologies represented by VR and AR have once again significantly propelled the advancement of HCI in recent years, especially in the area of user interfaces. For instance, one article\cite{paper10} clearly points out that immersive realities can provide personalized experiences and customized digital content in virtual heritage, which helps to establish contextual relationships between users and cultural backgrounds, and enhances user participation in virtual environments.

Furthermore, related researchers have been discussing how to design and evaluate virtual reality systems based on user needs and experiences, following a human-centered design principle. This continues the development and theoretical support of HCI in virtual reality technology.

\subsection{Spatial Computing of VR}
In the field of computer graphics(CG), we have transcended typical two-dimensional depictions, such as photos or video-quality flat images that can reach resolutions as high as 8K or 12K. While it has not reached the limits of human visual perception, CG industry already possess significant technological capabilities in the image display, which supports the applications of image and video display in our daily lives. In terms of flat image representation or what is viewed through a display screen, our current planar vision is nearly indistinguishable from reality. The counterpart to planar computation is spatial computation, which involves three-dimensional computations and the calculations related to their surrounding environment.

The emergence of the concept of spatial computing is defined in recorded literature as "Human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces."\cite{paper11e} The author emphasizes that objects in spatial computing first need to have value in the real world. Spatial computing relies on objects that already exist in the real world. Unlike in the realms of 3D modeling or digital design, the target objects of spatial computing need real or similar objects as references. In the computational process, not only the position and relationship within the Cartesian three-dimensional coordinate system are considered, but also the representation information of the surrounding environment of the object. Hence, the advancement of spatial computing lies in emphasizing the physicality and spatiality in computation. It no longer treats space as an abstract concept, but as the primary influencing target. This can better combine computers with our reality and has some connections with Digital Twin to some extent.

Thus, as mentioned in the discussions of HCI research professors, mixed reality is just a popular term. The underlying paradigm of mixed reality, spatial computing, is a more valuable concept within the field of HCI. The reason why spatial computing methods represented by VR and AR differ from previous interaction methods, as mentioned in their discussions, is that spatial computing is no longer a two-dimensional representation method.

The relationship between spatial computing and virtual reality technology is complementary. Spatial computing is considered the theoretical starting point of virtual reality technology, while technologies like VR, AR, and MR are considered the current manifestations of the concept of spatial computing in computer science. Users can process and understand information through interaction with the environment, and virtual reality can detect changes in space through various types of sensors and make real-time adjustments, thus providing users with a more intuitive and enriched interactive experience.

\subsection{The Development of VR Technology}
Compared to spatial computing, virtual reality technologies, symbolized by VR, may be more widely known by the general public. In a broad sense, virtual reality encompasses various manifestations, including VR, AR, and MR. All these forms involve interaction methods between real and virtual spaces and utilize spatial computing at their core. As the main topic of this paper is centered around VR, the two related technologies of AR and MR will not be further detailed and discussed here.

Virtual reality technology is the most representative of its kind and the first to have been proven to confer significant benefits to computer development and human societal advancement. There are many definitions of VR. For instance, VR is described as "an interactive and immersive (with the feeling of presence) experience in a simulated (autonomous) world."\cite{paper14} Additionally, similar yet not entirely identical definitions note three features of VR: high immersion, high environmental perception, and high environmental interaction\cite{paper15}. According to these three factors, VR can be classified into three different levels of systems: Desktop VR, Fish Tank VR, and Immersive Systems.

Although the developmental history of virtual reality technology is not explicitly laid out, its milestone developments represent different levels of VR technology. The earliest can be traced back to 1962 when the Sensorama simulator first provided a comprehensive sensory experience that included audio, smell, and touch. In 1968, the Sword of Damocles was regarded as the first VR head-mounted display, followed by commercialized VR devices such as the Dataglove, EyePhone, and Boom in the 1980s. With the advancement and cost reduction of VR technology, modern VR headsets represented by Oculus, HTC VIVE, and PlaystationVR emerged after 2010. To date, as the technology has evolved, the application fields have gradually expanded from gaming and entertainment to education, training, healthcare, socialization, and more. Various innovative technologies related to it, such as eye tracking, gesture recognition, full-body tracking, and even artificial intelligence, are making the VR experience more realistic and immersive.

As pointed out in a report analyzing the popular VR application BeatSaber, user observation and interaction with target objects are gradually shifting from flat screen layouts to surrounding spaces. The article summarizes in a straightforward manner the contributing factors to the sense of reality in virtual technology and the future development direction of virtual technology, which is not confined to a singular visual perspective.

\section{Virtual Reality and Haptic}
\subsection{Multisensory and Synesthesia}

When virtual reality technology was defined, it was emphasized that the sense of immersion in virtual space is influenced not only by visual factors. The definition of spatial computing also underlines that apart from the target object itself, the environment around the target object should also be included in the calculation, including hearing, touch, smell, and so on. A report\cite{paper16} indicates that additional sensory input can enhance the sense of presence in a virtual environment and improve memory of objects within the virtual environment. Adding sensory cues beyond vision may be an effective way to manage the trade-off between the level of detail present in the virtual environment and the frame rate.

The method of providing users with multi-sensory information is referred to in cognitive psychology as Multisensory and Synesthesia. Multisensory and Synesthesia are two similar but not identical concepts. According to relevant neuroscience research\cite{paper17e}, the cooperation or interaction between multiple senses and the fusion of their information content is called multisensory integration, thereby creating a cross-modal stimulus combination. This combination is used to achieve a certain type of response. The latency of this multi-sensory response is significantly shorter than that of any single sensory response. In addition, multi-sensory information sources can enhance or even create a single perceptual experience, similar in effect to synesthesia.

A significant reference\cite{paper18e} in the field of synesthesia interprets synesthesia as a "joined sensation". This specifically manifests when the experience of one sense involuntarily transforms into another sensory experience. For example, in the eyes of people with synesthesia, seeing numbers also assigns colors to these numbers. Tasting and smelling simultaneously enables them to sense additional information such as an object's contour, texture, weight, and temperature. Like multisensory, establishing artificial synesthesia in a virtual reality environment through multimodal methods can significantly enhance immersive VR's performance in system guidance and directing user attention\cite{paper19}.

\subsection{The Necessity of Haptic Technology}
Both spatial computing and multisensory experiences, these leading research trends indicate that in the future development of VR, traditional human-computer interaction that merely uses planar graphic interaction and audio-visual information will not suffice to express the sense of immersion required by virtual reality technology. Several articles\cite{paper20}\cite{paper21} mention that haptic feedback is essential to enhance the immersion and interactivity of VR systems. At present, major commercial VR games or VR videos only provide good visual and haptic feedback, while the perception of haptic information is primarily through vibration.

In addition to enriching perceptual capabilities, another necessity for the development of haptic sensation is its initiative. In other words, haptic sensation should be actively emitted for a certain purpose or behavior. Put differently, haptic information is not only input information received by the user, but also carries input functions different from vision and hearing. The information contained therein varies depending on the user's usage scenarios and intentions. How to analyze integrated information and select the needed haptic information for users\cite{paper22} is one of the main topics in the development of haptics.

Moreover, in classic multimodal structures, haptics serves as a bridge linking various perceptual information. For instance, it conveys the shape, size, roughness, temperature, and so forth of an object to the user through tactile touches and grasps. This interaction process often requires users to actively choose. In general, in a complete multimodal perception structure, multimodal perception needs haptic to provide an information source to enhance the quality and depth of multimodality.

\subsection{Development and Application of Haptic Technology}
Similar to human-computer interaction, the development of haptic technology can also be roughly divided into three stages\cite{paper20} based on the development of personal computer technology:

\begin{enumerate}
    \item \textbf{Desktop Haptics}: Desktop haptics is mainly used in desktop environments, typically provided by a mechanical lever with a certain degree of freedom to provide feedback of some real physical touch sensation, such as the shape or resistance of the target object. The design goal mainly focuses on surgical simulation or mechanical operation training.

    \item \textbf{Surface Haptics}: Surface haptics primarily targets platforms such as smartphones and tablets, dedicating to creating direct contact information between fingers and object surfaces, for instance, fingers can feel the contour and roughness of the object surface. An example of this is the mechanical vibration widely applied on mobile phones today. When pressing a button on the screen, the phone generates corresponding vibration, thereby simulating the interactive behavior between the finger and the real button.

    \item \textbf{Wearable Haptics}: In recent years, with the development of VR technology and the gradual popularization of commercial VR equipment, how to provide force feedback for the entire hand in the VR space, and support more detailed operations like touching, grasping, and manipulation through different gestures under the full freedom of fingers, has become a pressing question. The main research trend at present is haptic gloves. Major technology companies have launched their versions, but the specific features and standard requirements for force feedback gloves have not been clearly defined. In addition to the more idealized haptic glove solutions, there are many haptic schemes that have chosen fingertip feedback devices, handheld feedback devices, and controller-type force feedback devices. However, they all share the same goal: to provide users with gesture interactions in virtual space that are as similar as possible to those in real life.
\end{enumerate}

\section{Haptic and Automation}
\subsection{Telexistence and Artificial Intelligence}
Virtual reality technology not only can be used to create virtual scenes for users, but its high immersion and interactivity can also transmit user scenarios across space. For instance, users can achieve remote work at home using head-mounted displays. The combination of virtual reality technology and robotics can achieve a physical presence. Especially during the COVID-19 outbreak period, which significantly impacted users' activities and contact behaviors, many researchers are committed to combining VR, haptics, and robotics to establish a telesensation control system, also known as remote presence or telexistence. 

Differing from previous remote presentation systems\cite{paper23}, modern remote presence systems actively use multisensory sensors. They not only provide an immersive experience in terms of audio-visual effects but also generate actual physical interactions, such as touching, manipulating, and grasping, amongst other complex movements. The specific forms of manifestation are no longer limited to humanoid robotic devices. There are more flexible forms of existence, such as soft robots, robotic arms, and even wearable types\cite{paper24}.

Additionally, when discussing robot manufacturing, we cannot ignore artificial intelligence, which acts like the brain of a robot. As a core technology with robots, it is not surprisingly being attempted in combination with VR\cite{paper25} or remote presence technology, acting as a control solution when not directly controlled by a user. In the classic artificial intelligence (AI) theory reference\cite{paper26}, AI is defined as "the study of [intelligent] agents that receive precepts from the environment and take action. Each such agent is implemented by a function that maps percepts to actions, and we cover different ways to represent these functions, such as production systems, reactive agents, logical planners, neural networks, and decision-theoretic systems" emphasizing perception and action as two key components. These two components have been repeatedly mentioned in previous descriptions of virtual reality technology and haptic technology. Therefore, when developing remote presence technology, we inevitably need to consider the role that AI plays in remote presence technology and the interaction between users and AI.

\subsection{The Sense of Agency}
With the development of automation and AI, the process of HCI is no longer a single control behavior. The computer will complete the transition from a tool to an intelligent tool. The biggest convenience of technological development is to simplify complex things, such as breaking through the constraints of space and time, making remote things close, and preserving things from a long time ago until now. While simplifying or automating processes improves task efficiency, it also brings many issues, one of which is decision transparency.

Decision transparency refers to the clarity and comprehensibility of the decision-making process. In the context of AI-driven automation industries, it concerns whether the behavior of the current device is determined by AI or by the user. In other words, when users use automated devices, is it the AI system manipulating the entire behavior process, the user themselves, or a joint decision between the user and the AI system? As mentioned in several articles\cite{paper27}, agency is a fundamental and ongoing basis for the interaction between users and the world.

Some studies have already proven that the combination of multisensory elements (e.g., visuomotor and visuotactile) can enhance users' sense of ownership of their virtual bodies in virtual reality\cite{paper28}. However, when users collaborate with AI systems, which also have a strong sense of agency, whether haptics will similarly enhance the connection between users and automated devices, and whether it can enhance the sense of agency, is the main topic of discussion in this paper.

%\subsection{Haptic and AI}

\section{Research Objectives}
\subsection{Research Objectives}
The research in this paper begins with the degree to which users perceive motion in virtual space. After multiple trials and tests, the research target is defined as the moving object in the virtual space, and the perception of motion is precisely targeted to the state of motion of self-moving objects.

We hope that through the mechanical device designed in this research, we can accurately convey to users the mechanical force feedback on the motion state of self-moving objects, including common displacement changes, angle changes and speed changes. This could address the insufficiency of force feedback for the state of moving objects in virtual space. After providing effective mechanical force feedback, we consider whether conveying the motion state of automated devices in an automated operating environment using an AI system can enhance the user experience and sense of agency. The aim is to explore the feasibility of implementing rich haptic technology and whether effective rich haptic technology can enhance the interaction between users and intelligent systems.

\subsection{Iteration of the research objectives}
The goals of this research were not precisely defined in the early stages of the project. With the core ideas and keywords (virtual space, spatial computation, haptic feedback, motion perception) fixed, the research objectives were refined or reinterpreted, including the following phased research objectives or research questions:

\begin{enumerate}
    \item Researching the design of a haptic device for delivering force feedback of the parrying actions to users in virtual space.
    \item Exploring how to simplify haptic devices for conveying physical cues to users and exploring expanded application scenarios.
    \item Researching and analyzing current trends in haptic devices in terms of research and design, and exploring solutions that align with the research objectives.
    \item Researching the design of a haptic device for conveying the motion state of moving objects in virtual space to users and validating its effectiveness.
    \item Investigating whether the use of haptic devices for motion state cues affects the user experience and interaction patterns with automated devices equipped with an AI system.
\end{enumerate}

%\subsection{Theoretical Model}